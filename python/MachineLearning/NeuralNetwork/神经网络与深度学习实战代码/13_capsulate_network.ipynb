{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_capsulate_network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "5hIsN0mYE1q8",
        "colab_type": "code",
        "outputId": "f6b9e896-152b-4891-bd47-45ee7997a3d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uwWuTePWEBYJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_image_dir = '/content/gdrive/My Drive/fashin-mnist/train-images-idx3-ubyte'\n",
        "train_label_dir = '/content/gdrive/My Drive/fashin-mnist/train-labels-idx1-ubyte'\n",
        "test_image_dir = '/content/gdrive/My Drive/fashin-mnist/t10k-images-idx3-ubyte'\n",
        "test_label_dir = '/content/gdrive/My Drive/fashin-mnist/t10k-labels-idx1-ubyte'\n",
        "\n",
        "N_CLASSES = 10\n",
        "IMG_WIDTH = 28\n",
        "IMG_HEIGHT = 28\n",
        "N_CHANNELS = 1 #图片是灰度图\n",
        "IMAGE_LABELS = ['T-shirt/top', 'Trousr', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "加载图片数据集，将60000张图片分成两部分，第一部分55000张用于训练，\n",
        "第二部分5000张用于校验\n",
        "'''\n",
        "def  load_data(load_type = 'train'):\n",
        "  #构造两部分数据x对应图片，y对应标签\n",
        "  if load_type == 'train':\n",
        "    image_file = open(train_image_dir)\n",
        "    image_data = np.fromfile(file = image_file, dtype = np.uint8)\n",
        "    x = image_data[16:].reshape((60000, 28, 28, 1)).astype(np.float32)\n",
        "    \n",
        "    label_file = open(train_label_dir)\n",
        "    label_data = np.fromfile(label_file, dtype = np.uint8)\n",
        "    y = label_data[8:].reshape(60000).astype(np.int32)\n",
        "    \n",
        "    #把像素点的值转换到[0,1]之间\n",
        "    x_train= x[: 55000] / 255.\n",
        "    y_train = y[:55000]\n",
        "    '''\n",
        "    假设y_train[0] = 4,下面代码把y_train[0]变成一个向量\n",
        "    [0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0]\n",
        "    '''\n",
        "    y_train = (np.arange(N_CLASSES) == y_train[:, None]).astype(np.float32)\n",
        "    \n",
        "    x_valid = x[55000 :,] / 255.\n",
        "    y_valid = y[55000 :,]\n",
        "    y_valid = (np.arange(N_CLASSES) == y_valid[:, None]).astype(np.float32)\n",
        "    return x_train, y_train, x_valid, y_valid\n",
        "  elif load_type == 'test':\n",
        "    image_file = open(test_image_dir)\n",
        "    image_data = np.fromfile(file = image_file, dtype = np.uint8)\n",
        "    x_test = image_data[16:].reshape((10000, 28 , 28, 1)).astype(np.float32)\n",
        "    \n",
        "    label_file = open(test_label_dir)\n",
        "    label_data = np.fromfile(file = label_file, dtype = np.uint8)\n",
        "    y_test = label_data[8:].reshape(10000).astype(np.int32)\n",
        "    y_test = (np.arange(N_CLASSES) == y_test[:, None]).astype(np.float32)\n",
        "    return x_test / 255., y_test\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qL1OOXIeEFtG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "根据公式（2）计算squash\n",
        "'''\n",
        "def  squash(vectors, name = None):\n",
        "  with tf.name_scope(name, default_name = 'squash_op'):\n",
        "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis=-2, keepdims = True)\n",
        "    #tf.keras.backend.epsilon() = 1e-7，加上这个值是防止s_squared_norm为0时产生除0异常\n",
        "    scale = s_squared_norm / (1. + s_squared_norm) / tf.sqrt(s_squared_norm + \n",
        "                                                            tf.keras.backend.epsilon())\n",
        "    return scale * vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7-MmpgRYELL6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#胶囊层含有32个胶囊\n",
        "MAPS_CAPS1 = 32\n",
        "#图12-34胶囊层每个正方体对应的宽和高都是6\n",
        "NCAPS_CAPS1 = MAPS_CAPS1*6*6\n",
        "#胶囊层每个胶囊对上一层数据进行9*9分割后，识别结果为含有8个元素的向量\n",
        "CAPS_DIM_CAPS1 = 8\n",
        "#第二层胶囊层的胶囊个数\n",
        "NCAPS_CAPS2 = 10\n",
        "#第二层每个胶囊输出结果为含有16个元素的向量\n",
        "CAPS_DIM_CAPS2 = 16\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "ROUTING_ITERATIONS = 3\n",
        "STDEV = 0.01\n",
        "'''\n",
        "根据路由流程，把上一层胶囊的输出结果传递给下一层胶囊\n",
        "'''\n",
        "def  routing(u):\n",
        "  #把上一层编号为i的胶囊与下一层编号为j的胶囊之间的连接系数初始化为0\n",
        "  b_ij = tf.zeros([BATCH_SIZE, NCAPS_CAPS1, NCAPS_CAPS2, 1, 1],\n",
        "                 dtype = np.float32, name=\"b_ij\")\n",
        "  '''\n",
        "  u是第二层卷积层的输出结果，它的格式为[BATCH_SIZE, 1152, 1, 8, 1], 其中1152对应’胶囊‘个数，\n",
        "  其中后面的[8,1]对应胶囊输出向量，中间的1用于方便运算，下面代码将u扩展为[BATCH_SIZE, 1152, 10, 8, 1]\n",
        "  '''\n",
        "  u = tf.tile(u, [1, 1, b_ij.shape[2].value, 1, 1])\n",
        "  #构造公式（1）所对应的W[i,j]其中i=1152, j = 10\n",
        "  W = tf.get_variable('W', shape = (1, u.shape[1].value, b_ij.shape[2].value,\n",
        "                                   u.shape[3].value, CAPS_DIM_CAPS2), dtype = tf.float32,\n",
        "                     initializer = tf.random_normal_initializer(stddev = STDEV))\n",
        "  #将W扩展为[BATCH_SIZE, 1152, 10, 8, 6]\n",
        "  W = tf.tile(W, [BATCH_SIZE, 1, 1, 1, 1])\n",
        "  #根据公式(1)上半部分计算u^\n",
        "  u_hat = tf.matmul(W, u, transpose_a = True)\n",
        "  '''\n",
        "  由于u_hat_stopped用于计算c(i,j)，因为后者通过迭代运算而来，因此不参与到网络的参数调整中，\n",
        "  因此网络用梯度下降法调整参数时，由u_hat参与迭代运算产生的变量都不能进行调整，\n",
        "  所以要stop_greadient\n",
        "  '''\n",
        "  u_hat_stopped = tf.stop_gradient(u_hat, name = 'gradient_stop')\n",
        "  \n",
        "  #根据公式(1)第二部分计算Sj\n",
        "  for r in range(ROUTING_ITERATIONS):\n",
        "     with tf.variable_scope('iterations_' + str(r)):\n",
        "      c_ij = tf.nn.softmax(b_ij, axis = 2)\n",
        "    \n",
        "      if r == ROUTING_ITERATIONS - 1:\n",
        "        #s_j的维度为[BATCH_SIZE, 1152, 10, 16, 1]\n",
        "        s_j = tf.multiply(c_ij, u_hat)\n",
        "        #计算∑(c_ij*u_hat_i)\n",
        "        s_j = tf.reduce_sum(s_j, axis = 1, keep_dims = True)\n",
        "        v_j = squash(s_j)\n",
        "      elif r < ROUTING_ITERATIONS - 1:\n",
        "        '''\n",
        "        这里的变量处于迭代运算的中间阶段，因此不用参与到网络的参数调整流程\n",
        "        '''\n",
        "        s_j = tf.multiply(c_ij, u_hat_stopped)\n",
        "        s_j = tf.reduce_sum(s_j, axis = 1, keepdims = True)\n",
        "        v_j = squash(s_j)\n",
        "        v_j = tf.tile(v_j, [1, u.shape[1].value, 1, 1, 1])\n",
        "        u_hat_dot_v_j = tf.matmul(u_hat_stopped, v_j, transpose_a = True)\n",
        "        b_ij = tf.add(b_ij, u_hat_dot_v_j)\n",
        "      \n",
        "  \n",
        "  return  tf.squeeze(v_j, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fv-RVB2XG76L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "CONV1_LAYER_PARAMS= {\n",
        "    \"filters\": 256,\n",
        "    \"kernel_size\": 9,\n",
        "    \"activation\": tf.nn.relu,\n",
        "    \"padding\": \"valid\",\n",
        "    \"strides\":1\n",
        "}\n",
        "\n",
        "CONV2_LAYER_PARAMS = {\"filters\": MAPS_CAPS1 * CAPS_DIM_CAPS1,\n",
        "                     \"kernel_size\": 9,\n",
        "                     \"strides\": 2,\n",
        "                     \"padding\": \"valid\",\n",
        "                     \"activation\": tf.nn.relu}\n",
        "\n",
        "\n",
        "\n",
        "#构建胶囊网络\n",
        "class CapsNet:\n",
        "  def  __init__(self):\n",
        "    with tf.variable_scope('Input'):\n",
        "      #输入图片数据,None对应于BATCH_SIZE，也就是一次输入的图片数量\n",
        "      self.X = tf.placeholder(shape=[None, IMG_WIDTH, IMG_HEIGHT, N_CHANNELS],\n",
        "                             dtype = tf.float32, name = 'X')\n",
        "      self.Y = tf.placeholder(shape = [None, N_CLASSES], dtype = tf.float32, name = \"Y\")\n",
        "      self.mask_with_labels = tf.placeholder_with_default(False, shape = (),\n",
        "                                                        name = 'mask_with_labels')\n",
        "      \n",
        "      self.define_network()\n",
        "      self.define_loss()\n",
        "      self.define_accuracy()\n",
        "      self.define_optimizer()\n",
        "      self.summary()\n",
        "      \n",
        "  def  define_network(self):\n",
        "    with tf.variable_scope('Conv1_layer',  reuse = tf.AUTO_REUSE):\n",
        "      '''\n",
        "      第一层是含有一个神经元的卷积网络，它将输入图片进行9*9的小块分割,每次分割偏移一个单位，\n",
        "      每小块识别后输出长度为256的向量，一个28*28规格的图片可以分割出20*20个小块，它最终\n",
        "      输出的数据格式为[BATCH_SIZE,20, 20, 256]\n",
        "      '''\n",
        "      conv1_layer = tf.layers.conv2d(self.X, name = \"conv1_layer\", **CONV1_LAYER_PARAMS)\n",
        "    \n",
        "    with tf.variable_scope('PrimaryCaps_layer', reuse = tf.AUTO_REUSE):\n",
        "      '''\n",
        "      第二层卷积层接收第一层卷积层的输出结果。然后对第一层的输出进行9*9分割，每次分割偏移2个单位，\n",
        "      由此产生6*6=36个小块，每个小块做卷积运输后输出长度为256的向量，由此得到它的输出为[6,6,256],\n",
        "      在逻辑上它对应于[32, 6,6, 8],也就是图12-35中间部分所表示的多个大方块。它又可以再次\n",
        "      从逻辑上看看做是[1152, 8, 1]，由此对应1152个胶囊\n",
        "      '''\n",
        "      conv2_layer = tf.layers.conv2d(conv1_layer, name = \"conv2_layer\", **CONV2_LAYER_PARAMS)\n",
        "      #将输出格式转换为[BATCH_SIZE, 1152, 8, 1]\n",
        "      primary_caps = tf.reshape(conv2_layer, (BATCH_SIZE, NCAPS_CAPS1, CAPS_DIM_CAPS1, 1),\n",
        "                               name = 'primary_caps')\n",
        "      primary_caps_output = squash(primary_caps, name = 'caps1_output')\n",
        "      \n",
        "    with tf.variable_scope('DigitCaps_layer', reuse = tf.AUTO_REUSE):\n",
        "      '''\n",
        "      使用路由算法把第二层胶囊结果传递给第三层胶囊\n",
        "      '''\n",
        "      #把第二层胶囊层输出的数据格式由[BATCH_SIZE, 1152, 8, 1]转换成[BATCH_SIZE, 1152, 1, 8, 1]便于运输\n",
        "      digitcaps_input = tf.reshape(primary_caps_output, shape=(BATCH_SIZE, NCAPS_CAPS1,\n",
        "                                                              1, CAPS_DIM_CAPS1, 1))\n",
        "      #输出第三层胶囊层10个胶囊对应向量其格式为[BATCH_SIZE, 10, 16 , 1]\n",
        "      self.digitcaps_output = routing(digitcaps_input)\n",
        "      \n",
        "    with tf.variable_scope('Masking', reuse = tf.AUTO_REUSE):\n",
        "      #计算第三层胶囊层每个胶囊向量长度，该长度表示输入图片属于对应分类的概率\n",
        "      self.v_norm = tf.sqrt(tf.reduce_sum(tf.square(self.digitcaps_output), axis = 2,\n",
        "                                          keepdims = True) + tf.keras.backend.epsilon())\n",
        "      #[batch_size, 10, 1, 1]\n",
        "      predicted_class = tf.to_int32(tf.argmax(self.v_norm, axis = 1))\n",
        "      #转换为[batch_size]\n",
        "      self.y_predicted = tf.reshape(predicted_class, shape = (BATCH_SIZE, ))\n",
        "      #转换成[batch_size, 10]为重构图片做准备\n",
        "      y_predicted_one_hot = tf.one_hot(self.y_predicted, depth = NCAPS_CAPS2)\n",
        "      \n",
        "      '''\n",
        "      如果当前网络处于训练状态self.mask_with_labels == True，那么使用输入图片对应的标签选出第三层胶囊层对应的胶囊向量。\n",
        "      如果当前处于测试状态self.mask_with_labels == False,那么选出长度最大的胶囊向量\n",
        "      '''\n",
        "      reconstruction_targets = tf.cond(self.mask_with_labels,\n",
        "                                      lambda: self.Y,\n",
        "                                      lambda: y_predicted_one_hot,\n",
        "                                      name = 'reconstruction_targets')\n",
        "      '''\n",
        "      把除选中的胶囊之外的其他胶囊设置为0\n",
        "      '''\n",
        "      digitcaps_output_masked = tf.multiply(tf.squeeze(self.digitcaps_output),\n",
        "                                           tf.expand_dims(reconstruction_targets, -1))\n",
        "      #把digitcaps_output_masked格式由[BATCH_SIZE, 10, 16]转换为[BATCH_SIZE, 160]\n",
        "      decoder_input = tf.reshape(digitcaps_output_masked, [BATCH_SIZE, -1])\n",
        "      \n",
        "    with tf.variable_scope('Decoder', reuse = tf.AUTO_REUSE):\n",
        "      #构造最后三个全连接层\n",
        "      layer1_size = 512\n",
        "      layer2_size = 1024\n",
        "      output_size = IMG_WIDTH * IMG_HEIGHT\n",
        "      \n",
        "      fc1 = tf.layers.dense(decoder_input, layer1_size, activation = tf.nn.relu,\n",
        "                           name = 'FC1')\n",
        "      fc2 = tf.layers.dense(fc1, layer2_size, activation = tf.nn.relu,\n",
        "                           name = 'FC2')\n",
        "      self.decoder_output = tf.layers.dense(fc2, output_size, activation = tf.nn.sigmoid,\n",
        "                                           name = 'FC3')\n",
        "      \n",
        "  def  define_loss(self):\n",
        "    #实现边缘损失\n",
        "    with  tf.variable_scope('Margin_loss', reuse=tf.AUTO_REUSE):\n",
        "      positive_error = tf.square(tf.maximum(0., 0.9 - self.v_norm))\n",
        "      negative_error = tf.square(tf.maximum(0., self.v_norm - 0.1))\n",
        "      \n",
        "      positive_error = tf.reshape(positive_error, shape = (BATCH_SIZE, -1))\n",
        "      negative_error = tf.reshape(negative_error, shape = (BATCH_SIZE, -1))\n",
        "      \n",
        "      loss_vec = self.Y * positive_error + 0.5*(1 - self.Y)*negative_error\n",
        "      self.margin_loss = tf.reduce_mean(tf.reduce_sum(loss_vec, axis = 1), \n",
        "                                       name = 'margin_loss')\n",
        "      \n",
        "    with  tf.variable_scope('Reconstruction_loss', reuse=tf.AUTO_REUSE):\n",
        "      ground_truth = tf.reshape(self.X, shape = (BATCH_SIZE, -1))\n",
        "      self.reconstruction_loss = tf.reduce_mean(tf.square(self.decoder_output\n",
        "                                                         - ground_truth))\n",
        "    with  tf.variable_scope('Combine_Loss'):\n",
        "      self.combined_loss = self.margin_loss + 0.0005 * self.reconstruction_loss\n",
        "      \n",
        "  def  define_accuracy(self):\n",
        "    with  tf.variable_scope('Accuracy', reuse=tf.AUTO_REUSE):\n",
        "      correct_predictions = tf.equal(tf.to_int32(tf.argmax(self.Y, axis = 1)),\n",
        "                                   self.y_predicted)\n",
        "      self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "      \n",
        "  def  define_optimizer(self):\n",
        "    with  tf.variable_scope('Optimizer', reuse=tf.AUTO_REUSE):\n",
        "      optimizer = tf.train.AdamOptimizer()\n",
        "      self.train_optimizer = optimizer.minimize(self.combined_loss, name = \n",
        "                                               'trainning_optimizer')\n",
        "      \n",
        "  def  summary(self):\n",
        "    reconstructed_image = tf.reshape(self.decoder_output, shape = (BATCH_SIZE,\n",
        "                                                                  IMG_WIDTH, IMG_HEIGHT,\n",
        "                                                                  N_CHANNELS))\n",
        "    summary_list = [tf.summary.scalar('Loss/margin_loss', self.margin_loss),\n",
        "                   tf.summary.scalar('Loss/reconstruction_loss', self.reconstruction_loss),\n",
        "                   tf.summary.image('original', self.X),\n",
        "                   tf.summary.image('reconstructed', reconstructed_image)]\n",
        "    self.summary = tf.summary.merge(summary_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l5spEyf7oWNu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "CHECKPOINT_DIR = '/content/gdrive/My Drive/capsulate_model'\n",
        "LOG_DIR = '/content/gdrive/My Drive/capsulate_model/logs/'\n",
        "RESTORE_PAINTING = False\n",
        "EPOCHS = 4\n",
        "\n",
        "def  train(model):\n",
        "  x_train, y_train, x_valid, y_valid = load_data(load_type = 'train')\n",
        "  print('Data set loaded')\n",
        "  num_batches = int(y_train.shape[0] / BATCH_SIZE)\n",
        "  if not os.path.exists(CHECKPOINT_DIR):\n",
        "    os.makedirs(CHECKPOINT_DIR)\n",
        "  \n",
        "  with  tf.Session() as sess:\n",
        "    if RESTORE_PAINTING:\n",
        "      #将存储在磁盘上的网络参数加载进来\n",
        "      saver = tf.train.Saver()\n",
        "      ckpt = tf.train.get_checkpoint_state(CHECKPOINT_DIR)\n",
        "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "      print('Model Loaded')\n",
        "      start_epoch = int(str(ckpt.model_checkpoint_path).split('-')[-1])\n",
        "      train_file, val_file, best_loss_val = load_existing_details()     \n",
        "    else:\n",
        "      #初始化运算图准备开始训练\n",
        "      saver = tf.train.Saver(tf.global_variables())\n",
        "      tf.global_variables_initializer().run()\n",
        "      print('Initialization complete')\n",
        "      train_file, val_file = write_progress('train')\n",
        "      start_epoch = 0\n",
        "      best_loss_val = np.infty\n",
        "    \n",
        "    print('Training Starts with batches: ', num_batches)\n",
        "    acc_batch_all = loss_batch_all = np.array([])\n",
        "    train_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
        "    #启动训练循环\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "      x_train, y_train = shuffle_data(x_train, y_train)\n",
        "      for step in range(num_batches):\n",
        "        start = step * BATCH_SIZE\n",
        "        end = (step + 1) * BATCH_SIZE\n",
        "        global_step = epoch * num_batches + step\n",
        "        x_batch, y_batch = x_train[start : end], y_train[start : end]\n",
        "        feed_dict_batch = {model.X: x_batch, model.Y: y_batch, model.mask_with_labels: True}\n",
        "        \n",
        "        if not (step % 100):\n",
        "          #一次输入128张图片启动训练\n",
        "          _, acc_batch, loss_batch, summary_ = sess.run([model.train_optimizer,\n",
        "                                                        model.accuracy,\n",
        "                                                        model.combined_loss,\n",
        "                                                        model.summary],\n",
        "                                                       feed_dict = feed_dict_batch)\n",
        "          #统计训练后网络的相关信息\n",
        "          train_writer.add_summary(summary_, global_step)\n",
        "          acc_batch_all = np.append(acc_batch_all, acc_batch)\n",
        "          loss_batch_all = np.append(loss_batch_all, loss_batch)\n",
        "          mean_acc, mean_loss = np.mean(acc_batch_all), np.mean(loss_batch_all)\n",
        "          \n",
        "          summary_ = tf.Summary(value = [tf.Summary.Value(tag = 'Accuracy', \n",
        "                                                         simple_value = mean_acc)])\n",
        "          train_writer.add_summary(summary_, global_step)\n",
        "          \n",
        "          summary_ = tf.Summary(value = [tf.Summary.Value(tag = 'Loss/combined_loss',\n",
        "                                                         simple_value = mean_loss)])\n",
        "          train_writer.add_summary(summary_, global_step)\n",
        "          \n",
        "          print('write to train file:', str(global_step) + ',' + str(mean_acc) + ','\n",
        "                          + str(mean_loss))\n",
        "          \n",
        "          train_file.write(str(global_step) + ',' + str(mean_acc) + ','\n",
        "                          + str(mean_loss) + \"\\n\")\n",
        "          train_file.flush()\n",
        "          \n",
        "          print(\"  Batch #{0}, Epoch: #{1}, Mean Training loss: {2:.4f}, Mean Training accuracy: {3:.01%}\"\n",
        "               .format(step, (epoch + 1), mean_loss, mean_acc))\n",
        "          acc_batch_all = loss_batch_all = np.array([])\n",
        "          saver.save(sess, CHECKPOINT_DIR + '/model.tfmodel', global_step = epoch + 1)\n",
        "        else:\n",
        "          _, acc_batch, loss_batch = sess.run([model.train_optimizer, model.accuracy,\n",
        "                                              model.combined_loss],\n",
        "                                             feed_dict = feed_dict_batch)\n",
        "          acc_batch_all = np.append(acc_batch_all, acc_batch)\n",
        "          loss_batch_all = np.append(loss_batch_all, loss_batch)\n",
        "          \n",
        "      \n",
        "      acc_val, loss_val = eval_performance(sess, model, x_valid, y_valid)\n",
        "      val_file.write(str(epoch+1) + ',' + str(acc_val) + ',' + str(loss_val) + '\\n')\n",
        "      val_file.flush()\n",
        "      \n",
        "      print('begin new Epoc')\n",
        "      print('\\rEpoch {} mean Train Accuracy: {:.4f}%, Mean Val accuracy: {:.4f}% Loss: {:.6f}{}'\n",
        "           .format(epoch + 1, mean_acc * 100, acc_val * 100, loss_val,\n",
        "                  \"(improved)\" if loss_val < best_loss_val else \"\"))\n",
        "      if loss_val < best_loss_val:\n",
        "        saver.save(sess, CHECKPOINT_DIR + '/model.tfmodel', global_step = epoch + 1)\n",
        "        best_loss_val = loss_val\n",
        "    \n",
        "    train_file.close()\n",
        "    val_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7YbfAmgA9YwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RESULTS_DIR = '/content/gdrive/My Drive/capsulate_model/results'\n",
        "\n",
        "def  load_existing_details():\n",
        "  '''\n",
        "  将存储在磁盘上的网络训练信息数据进行加载\n",
        "  '''\n",
        "  train_path = RESULTS_DIR + '/' + 'train.csv'\n",
        "  val_path = RESULTS_DIR + '/' + 'validation.csv'\n",
        "  \n",
        "  f_val = open(val_path, 'r')\n",
        "  lines = f_val.readlines()\n",
        "  data = np.genfromtxt(lines[-1:], delimiter = ',')\n",
        "  \n",
        "  if len(data) == 0:\n",
        "    train_file, val_file = write_progress('train')\n",
        "    return train_file, val_file\n",
        "  \n",
        "  min_loss = np.min(data[1:, 2])\n",
        "  \n",
        "  train_file = open(train_path, 'a')\n",
        "  val_file = open(val_path, 'a')\n",
        "  \n",
        "  return train_file, val_file, min_loss\n",
        "\n",
        "def  write_progress(op_type = 'train'):\n",
        "  '''\n",
        "  把网络当前的训练信息存储到磁盘文件\n",
        "  '''\n",
        "  if not os.path.exists(RESULTS_DIR):\n",
        "    os.mkdir(RESULT_DIR)\n",
        "    \n",
        "  if op_type == 'train':\n",
        "    train_path = RESULTS_DIR + '/' + 'train.csv'\n",
        "    val_path = RESULTS_DIR + '/' + 'validation.csv'\n",
        "    \n",
        "    if os.path.exists(train_path):\n",
        "      os.remove(train_path)\n",
        "    if os.path.exists(val_path):\n",
        "      os.remove(val_path)\n",
        "      \n",
        "    train_file = open(train_path, 'w')\n",
        "    train_file.write('step, accuracy,loss\\n')\n",
        "    val_file = open(val_path, 'w')\n",
        "    val_file.write('epoch,accuracy,loss\\n')\n",
        "    return train_file, val_file\n",
        "  else:\n",
        "    test_path = RESULTS_DIR + '/test.csv'\n",
        "    if os.path.exists(test_path):\n",
        "      os.remove(test_path)\n",
        "    test_file = open(test_path, 'w')\n",
        "    test_file.write('accuracy,loss\\n')\n",
        "    return test_file\n",
        "\n",
        "def  shuffle_data(x, y):\n",
        "  '''\n",
        "  打乱训练数据排序以便用于网络重复训练\n",
        "  '''\n",
        "  perm = np.arange(y.shape[0])\n",
        "  np.random.shuffle(perm)\n",
        "  shuffle_x = x[perm, :, : ,:]\n",
        "  shuffle_y = y[perm]\n",
        "  \n",
        "  return shuffle_x, shuffle_y\n",
        "\n",
        "def eval_performance(sess, model, x, y):\n",
        "  '''\n",
        "  检验网络对校验数据集验证的准确率\n",
        "  '''\n",
        "  acc_all = loss_all = np.array([])\n",
        "  num_batches = int(y.shape[0] / BATCH_SIZE)\n",
        "  for batch_num in range(num_batches):\n",
        "    start = batch_num * BATCH_SIZE\n",
        "    end = start + BATCH_SIZE\n",
        "    x_batch, y_batch = x[start : end], y[start : end]\n",
        "    acc_batch, loss_batch, prediction_bath = sess.run([model.accuracy, model.combined_loss,\n",
        "                                                      model.y_predicted],\n",
        "                                                     feed_dict = {\n",
        "                                                         model.X: x_batch,\n",
        "                                                         model.Y: y_batch\n",
        "                                                     })\n",
        "    acc_all = np.append(acc_all, acc_batch)\n",
        "    loss_all = np.append(loss_all, loss_batch)\n",
        "    \n",
        "  return np.mean(acc_all), np.mean(loss_all)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HxfHc_c6EF56",
        "colab_type": "code",
        "outputId": "cbca0404-05a5-4516-90f8-97872beef742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "#启动训练流程\n",
        "model = CapsNet()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-bcca00595bb1>:43: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-5-c529149f74bb>:50: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From <ipython-input-6-bcca00595bb1>:73: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-bcca00595bb1>:102: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9LAnwDE7XExA",
        "colab_type": "code",
        "outputId": "68ae85eb-be9f-4a87-a759-a12c5c5128c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        }
      },
      "cell_type": "code",
      "source": [
        "print('Step1: Train')\n",
        "train(model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step1: Train\n",
            "Data set loaded\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/capsulate_model/model.tfmodel-3\n",
            "Model Loaded\n",
            "Training Starts with batches:  429\n",
            "write to train file: 1287,0.8203125,0.09240468591451645\n",
            "  Batch #0, Epoch: #4, Mean Training loss: 0.0924, Mean Training accuracy: 82.0%\n",
            "write to train file: 1387,0.895078125,0.0779246722906828\n",
            "  Batch #100, Epoch: #4, Mean Training loss: 0.0779, Mean Training accuracy: 89.5%\n",
            "write to train file: 1487,0.900625,0.07402920432388782\n",
            "  Batch #200, Epoch: #4, Mean Training loss: 0.0740, Mean Training accuracy: 90.1%\n",
            "write to train file: 1587,0.894921875,0.0775107615441084\n",
            "  Batch #300, Epoch: #4, Mean Training loss: 0.0775, Mean Training accuracy: 89.5%\n",
            "write to train file: 1687,0.894765625,0.07713816359639168\n",
            "  Batch #400, Epoch: #4, Mean Training loss: 0.0771, Mean Training accuracy: 89.5%\n",
            "begin new Epoc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-fe9ae4d65fbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step1: Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-54be91553768>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     92\u001b[0m       print('\\rEpoch {} mean Train Accuracy: {:.4f}%, Mean Val accuracy: {:.4f}% Loss: {:.6f}{}'\n\u001b[1;32m     93\u001b[0m            .format(epoch + 1, mean_acc * 100, acc_val * 100, loss_val,\n\u001b[0;32m---> 94\u001b[0;31m                   \"(improved)\" if loss_val < best_loss_val else \"\"))\n\u001b[0m\u001b[1;32m     95\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHECKPOINT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/model.tfmodel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'best_loss_val' referenced before assignment"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jU8emDWAePZw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reconstruction(x, y, decoder_output, y_pred, n_samples):\n",
        "    #使用网络重构图片\n",
        "    sample_images = x.reshape(-1, IMG_WIDTH, IMG_HEIGHT)\n",
        "    decoded_image = decoder_output.reshape([-1, IMG_WIDTH, IMG_WIDTH])\n",
        "\n",
        "    fig = plt.figure(figsize=(n_samples * 2, 3))\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(1, n_samples, i+ 1)\n",
        "        plt.imshow(sample_images[i], cmap=\"binary\")\n",
        "        plt.title(\"Label:\" + IMAGE_LABELS[np.argmax(y[i])])\n",
        "        plt.axis(\"off\")\n",
        "    fig.savefig(RESULTS_DIR + '/' + 'input_images.png')\n",
        "    plt.show()\n",
        "\n",
        "    fig = plt.figure(figsize=(n_samples * 2, 3))\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(1, n_samples, i + 1)\n",
        "        plt.imshow(decoded_image[i], cmap=\"binary\")\n",
        "        plt.title(\"Prediction:\" + IMAGE_LABELS[y_pred[i]])\n",
        "        plt.axis(\"off\")\n",
        "    fig.savefig(RESULTS_DIR + '/' + 'decoder_images.png')\n",
        "    plt.show()\n",
        "\n",
        "def test(model):\n",
        "    x_test, y_test = load_data(load_type='test')\n",
        "    print('Loaded the test dataset')\n",
        "    test_file = write_progress('test')\n",
        "    #将网络从存储在磁盘中的参数中恢复回来\n",
        "    saver = tf.train.Saver()\n",
        "    ckpt = tf.train.get_checkpoint_state(CHECKPOINT_DIR)\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "        print('Model Loaded')\n",
        "        #加载测试数据集，检测网络对测试数据判断的正确率\n",
        "        acc_test, loss_test = eval_performance(sess, model, x_test, y_test)\n",
        "        test_file.write(str(acc_test) + ',' + str(loss_test) + '\\n')\n",
        "        test_file.flush()\n",
        "        print('-----------------------------------------------------------------------------')\n",
        "        print(\"Test Set Loss: {0:.4f}, Test Set Accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
        "\n",
        "\n",
        "def reconstruct_sample(model, n_samples=5):\n",
        "    #根据网络最后的全连接层输出重构输入图片\n",
        "    x_test, y_test = load_data(load_type='test')\n",
        "    sample_images, sample_labels = x_test[:BATCH_SIZE], y_test[:BATCH_SIZE]\n",
        "    saver = tf.train.Saver()\n",
        "    ckpt = tf.train.get_checkpoint_state(CHECKPOINT_DIR)\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "        feed_dict_samples = {model.X: sample_images, model.Y: sample_labels}\n",
        "        decoder_out, y_predicted = sess.run([model.decoder_output, model.y_predicted],\n",
        "                                       feed_dict=feed_dict_samples)\n",
        "    reconstruction(sample_images, sample_labels, decoder_out, y_predicted, n_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R4b_Ukreeayy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(\"Step2: Testing the performance of model on the Test Set\")\n",
        "test(model)\n",
        "print (\"Step3: Raaeconstructing some sample images\")\n",
        "reconstruct_sample(model,n_samples =3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hEGhvjdXik6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "56a09bfa-9c79-456b-bfa2-489eb41531a3"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(\"Step2: Testing the performance of model on the Test Set\")\n",
        "test(model)\n",
        "print (\"Step3: Raaeconstructing some sample images\")\n",
        "reconstruct_sample(model,n_samples =3)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step2: Testing the performance of model on the Test Set\n",
            "Loaded the test dataset\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/capsulate_model/model.tfmodel-3\n",
            "Model Loaded\n",
            "-----------------------------------------------------------------------------\n",
            "Test Set Loss: 0.0901, Test Set Accuracy: 87.6%\n",
            "Step3: Raaeconstructing some sample images\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/capsulate_model/model.tfmodel-3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAACQCAYAAADHj+psAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuRA7byIiF2LnTUTkQuy8iYhc6H8QNVEQWNeFLAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x216 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAACQCAYAAADHj+psAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIJy8CSEkgHDyJoSQAPI/p19WrrhVysAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x216 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}