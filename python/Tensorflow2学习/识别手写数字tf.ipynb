{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0467d792f1dc256c793d370cee59531f961e826777ee61a5e454fc69acb25c662",
   "display_name": "Python 3.8.8 64-bit ('tensorflow': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 28, 28) (60000,)\nbatch: (128, 28, 28) (128,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Function Description:手写数字识别\n",
    "'''\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32)/255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y \n",
    "\n",
    "(x ,y), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "batch_size = 128\n",
    "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "db = db.map(preprocess).shuffle(10000).batch(batch_size)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.map(preprocess).batch(batch_size)\n",
    "\n",
    "db_iter = iter(db)\n",
    "sample = next(db_iter)\n",
    "print('batch:', sample[0].shape, sample[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_5 (Dense)              (None, 256)               200960    \n_________________________________________________________________\ndense_6 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndense_7 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndense_8 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_9 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 244,522\nTrainable params: 244,522\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " modle = Sequential([\n",
    "     layers.Dense(256, activation = tf.nn.relu),\n",
    "     layers.Dense(128, activation = tf.nn.relu),\n",
    "     layers.Dense(64, activation = tf.nn.relu),\n",
    "     layers.Dense(32, activation = tf.nn.relu),\n",
    "     layers.Dense(10, activation = tf.nn.relu)\n",
    " ])\n",
    " modle.build(input_shape=[None,28*28])\n",
    " modle.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/liangyi/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Adam(lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0 loss: 2.3076236248016357 0.10553194582462311\n",
      "0 100 loss: 0.9101220369338989 6.532830238342285\n",
      "0 200 loss: 0.9955911636352539 7.688383102416992\n",
      "0 300 loss: 0.7440605759620667 6.276527404785156\n",
      "0 400 loss: 0.7618370056152344 8.465690612792969\n",
      "0 test acc: 0.7437\n",
      "1 0 loss: 0.679572343826294 8.535686492919922\n",
      "1 100 loss: 0.6789568662643433 11.624359130859375\n",
      "1 200 loss: 0.674553394317627 7.913036346435547\n",
      "1 300 loss: 0.7738601565361023 8.893863677978516\n",
      "1 400 loss: 0.6247442960739136 9.057350158691406\n",
      "1 test acc: 0.7469\n",
      "2 0 loss: 0.6061739921569824 11.332883834838867\n",
      "2 100 loss: 0.706638514995575 14.089025497436523\n",
      "2 200 loss: 0.8583351969718933 8.625970840454102\n",
      "2 300 loss: 0.5021404027938843 13.022235870361328\n",
      "2 400 loss: 0.5824685096740723 12.232477188110352\n",
      "2 test acc: 0.7494\n",
      "3 0 loss: 0.7296172380447388 9.329782485961914\n",
      "3 100 loss: 0.7809584736824036 12.589255332946777\n",
      "3 200 loss: 0.7950228452682495 13.361106872558594\n",
      "3 300 loss: 0.6903210282325745 13.059016227722168\n",
      "3 400 loss: 0.5784095525741577 10.610483169555664\n",
      "3 test acc: 0.7537\n",
      "4 0 loss: 0.6463238596916199 10.538595199584961\n",
      "4 100 loss: 0.5753133296966553 13.174966812133789\n",
      "4 200 loss: 0.6658580303192139 15.068639755249023\n",
      "4 300 loss: 0.4726625680923462 13.488591194152832\n",
      "4 400 loss: 0.7125458717346191 18.33079719543457\n",
      "4 test acc: 0.7518\n",
      "5 0 loss: 0.6601481437683105 19.71207618713379\n",
      "5 100 loss: 0.6893429756164551 15.028730392456055\n",
      "5 200 loss: 0.618305504322052 12.887392044067383\n",
      "5 300 loss: 0.6609336137771606 16.1025333404541\n",
      "5 400 loss: 0.5113124847412109 15.867449760437012\n",
      "5 test acc: 0.7604\n",
      "6 0 loss: 0.7568010687828064 15.70648193359375\n",
      "6 100 loss: 0.6146144270896912 14.32061767578125\n",
      "6 200 loss: 0.6569646596908569 14.268770217895508\n",
      "6 300 loss: 0.6133902668952942 15.425474166870117\n",
      "6 400 loss: 0.5073916912078857 15.565401077270508\n",
      "6 test acc: 0.7576\n",
      "7 0 loss: 0.5767980813980103 13.992847442626953\n",
      "7 100 loss: 0.6624019145965576 12.00311279296875\n",
      "7 200 loss: 0.4557023048400879 17.320308685302734\n",
      "7 300 loss: 0.6387822031974792 16.366044998168945\n",
      "7 400 loss: 0.6827650666236877 13.848213195800781\n",
      "7 test acc: 0.7585\n",
      "8 0 loss: 0.5031838417053223 23.6035213470459\n",
      "8 100 loss: 0.5857324600219727 17.602916717529297\n",
      "8 200 loss: 0.5683059096336365 16.912353515625\n",
      "8 300 loss: 0.4074859023094177 20.41044807434082\n",
      "8 400 loss: 0.8018612265586853 15.026105880737305\n",
      "8 test acc: 0.748\n",
      "9 0 loss: 0.6600589156150818 13.258869171142578\n",
      "9 100 loss: 0.7623105049133301 21.631959915161133\n",
      "9 200 loss: 0.5640770196914673 22.029659271240234\n",
      "9 300 loss: 0.7166951298713684 15.233394622802734\n",
      "9 400 loss: 0.7524005174636841 23.72555923461914\n",
      "9 test acc: 0.7619\n",
      "10 0 loss: 0.5082741379737854 19.784053802490234\n",
      "10 100 loss: 0.765889048576355 16.231666564941406\n",
      "10 200 loss: 0.6868062019348145 16.09811782836914\n",
      "10 300 loss: 0.6029717326164246 20.034839630126953\n",
      "10 400 loss: 0.5336887836456299 20.291851043701172\n",
      "10 test acc: 0.8175\n",
      "11 0 loss: 0.40072351694107056 18.488035202026367\n",
      "11 100 loss: 0.51474928855896 20.954662322998047\n",
      "11 200 loss: 0.47584277391433716 33.942344665527344\n",
      "11 300 loss: 0.39605408906936646 25.54644012451172\n",
      "11 400 loss: 0.3847593665122986 26.141637802124023\n",
      "11 test acc: 0.8195\n",
      "12 0 loss: 0.40479138493537903 21.914072036743164\n",
      "12 100 loss: 0.5818845629692078 24.299917221069336\n",
      "12 200 loss: 0.5754275918006897 25.63247299194336\n",
      "12 300 loss: 0.3395763039588928 29.9927978515625\n",
      "12 400 loss: 0.380617618560791 25.21670913696289\n",
      "12 test acc: 0.8119\n",
      "13 0 loss: 0.42368170619010925 39.42933654785156\n",
      "13 100 loss: 0.5717428922653198 29.051471710205078\n",
      "13 200 loss: 0.45791444182395935 29.87921142578125\n",
      "13 300 loss: 0.32870805263519287 24.72884750366211\n",
      "13 400 loss: 0.5104920864105225 24.241565704345703\n",
      "13 test acc: 0.8235\n",
      "14 0 loss: 0.4537097215652466 27.942989349365234\n",
      "14 100 loss: 0.5526257157325745 31.86956787109375\n",
      "14 200 loss: 0.38905802369117737 29.829952239990234\n",
      "14 300 loss: 0.40025794506073 34.83460998535156\n",
      "14 400 loss: 0.566115140914917 25.507877349853516\n",
      "14 test acc: 0.8223\n",
      "15 0 loss: 0.4999903738498688 31.112442016601562\n",
      "15 100 loss: 0.5230051875114441 27.170001983642578\n",
      "15 200 loss: 0.37009263038635254 32.08478546142578\n",
      "15 300 loss: 0.4039308726787567 29.928335189819336\n",
      "15 400 loss: 0.4263627529144287 29.152240753173828\n",
      "15 test acc: 0.8226\n",
      "16 0 loss: 0.38470953702926636 32.44267654418945\n",
      "16 100 loss: 0.5602982640266418 32.89536666870117\n",
      "16 200 loss: 0.47341400384902954 38.599143981933594\n",
      "16 300 loss: 0.4559340476989746 38.265052795410156\n",
      "16 400 loss: 0.3939318060874939 36.725128173828125\n",
      "16 test acc: 0.8198\n",
      "17 0 loss: 0.4315892457962036 36.8882942199707\n",
      "17 100 loss: 0.5168586373329163 39.99065399169922\n",
      "17 200 loss: 0.3855530023574829 39.3247184753418\n",
      "17 300 loss: 0.4422340393066406 39.5928840637207\n",
      "17 400 loss: 0.3672330677509308 32.39786911010742\n",
      "17 test acc: 0.8175\n",
      "18 0 loss: 0.40541085600852966 28.797161102294922\n",
      "18 100 loss: 0.4613915681838989 33.79265213012695\n",
      "18 200 loss: 0.512080192565918 45.59913635253906\n",
      "18 300 loss: 0.329210489988327 37.578983306884766\n",
      "18 400 loss: 0.2508704364299774 47.87494659423828\n",
      "18 test acc: 0.8178\n",
      "19 0 loss: 0.4445202648639679 40.105621337890625\n",
      "19 100 loss: 0.4606141746044159 37.296730041503906\n",
      "19 200 loss: 0.3881174623966217 52.9279670715332\n",
      "19 300 loss: 0.3504258990287781 35.14425277709961\n",
      "19 400 loss: 0.3859344720840454 40.41656494140625\n",
      "19 test acc: 0.8175\n",
      "20 0 loss: 0.34668171405792236 39.940162658691406\n",
      "20 100 loss: 0.37569212913513184 51.764915466308594\n",
      "20 200 loss: 0.41659075021743774 30.923324584960938\n",
      "20 300 loss: 0.4701828956604004 47.69734573364258\n",
      "20 400 loss: 0.47857993841171265 65.02597045898438\n",
      "20 test acc: 0.8214\n",
      "21 0 loss: 0.27239373326301575 40.0059928894043\n",
      "21 100 loss: 0.3419583737850189 44.207950592041016\n",
      "21 200 loss: 0.45434385538101196 43.09284591674805\n",
      "21 300 loss: 0.43988168239593506 52.90409469604492\n",
      "21 400 loss: 0.3925265669822693 46.64353942871094\n",
      "21 test acc: 0.8208\n",
      "22 0 loss: 0.46733853220939636 30.42145538330078\n",
      "22 100 loss: 0.4251336455345154 68.08232879638672\n",
      "22 200 loss: 0.46569788455963135 41.08998489379883\n",
      "22 300 loss: 0.6177980899810791 50.035221099853516\n",
      "22 400 loss: 0.33315014839172363 42.15672302246094\n",
      "22 test acc: 0.823\n",
      "23 0 loss: 0.39674627780914307 52.54776382446289\n",
      "23 100 loss: 0.5076088309288025 58.87062072753906\n",
      "23 200 loss: 0.3893665373325348 46.91096496582031\n",
      "23 300 loss: 0.3579676151275635 64.22329711914062\n",
      "23 400 loss: 0.4130697250366211 32.50425720214844\n",
      "23 test acc: 0.8212\n",
      "24 0 loss: 0.4188355803489685 38.815940856933594\n",
      "24 100 loss: 0.3798854351043701 53.64528274536133\n",
      "24 200 loss: 0.3009587228298187 43.23265838623047\n",
      "24 300 loss: 0.24417951703071594 44.68134307861328\n",
      "24 400 loss: 0.2867894172668457 45.535614013671875\n",
      "24 test acc: 0.818\n",
      "25 0 loss: 0.36949074268341064 51.33949661254883\n",
      "25 100 loss: 0.40257906913757324 54.10717010498047\n",
      "25 200 loss: 0.3933735489845276 56.020023345947266\n",
      "25 300 loss: 0.3785778284072876 55.18645477294922\n",
      "25 400 loss: 0.3214108645915985 48.658992767333984\n",
      "25 test acc: 0.8196\n",
      "26 0 loss: 0.33800452947616577 39.216854095458984\n",
      "26 100 loss: 0.5021496415138245 56.352664947509766\n",
      "26 200 loss: 0.34827399253845215 56.49725341796875\n",
      "26 300 loss: 0.4107297658920288 64.60346221923828\n",
      "26 400 loss: 0.4687321186065674 46.996734619140625\n",
      "26 test acc: 0.8203\n",
      "27 0 loss: 0.34460142254829407 52.04522705078125\n",
      "27 100 loss: 0.48126253485679626 51.47704315185547\n",
      "27 200 loss: 0.3275626301765442 64.15685272216797\n",
      "27 300 loss: 0.39114081859588623 52.798683166503906\n",
      "27 400 loss: 0.40041399002075195 39.76366424560547\n",
      "27 test acc: 0.8118\n",
      "28 0 loss: 0.37186741828918457 79.75442504882812\n",
      "28 100 loss: 0.23495261371135712 65.02467346191406\n",
      "28 200 loss: 0.3413237929344177 61.556846618652344\n",
      "28 300 loss: 0.2728973925113678 67.21627044677734\n",
      "28 400 loss: 0.3856496810913086 67.90196228027344\n",
      "28 test acc: 0.8148\n",
      "29 0 loss: 0.3373359441757202 78.39097595214844\n",
      "29 100 loss: 0.3058011531829834 72.21519470214844\n",
      "29 200 loss: 0.2844039499759674 66.371337890625\n",
      "29 300 loss: 0.3337046205997467 64.96345520019531\n",
      "29 400 loss: 0.2913229763507843 60.9200325012207\n",
      "29 test acc: 0.8192\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    for step, (x, y) in enumerate(db):\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = modle(x)\n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "\n",
    "            loss = tf.reduce_mean(tf.losses.MSE(y_onehot, logits))\n",
    "            loss2 = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "            loss2 = tf.reduce_mean(loss2)\n",
    "\n",
    "        grads = tape.gradient(loss2, modle.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, modle.trainable_variables))\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', float(loss2), float(loss))\n",
    "    total_correct = 0\n",
    "    total_num = 0\n",
    "    for x, y in db_test:\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        logits = modle(x)\n",
    "        porb = tf.nn.softmax(logits, axis=1)\n",
    "        pred = tf.argmax(porb, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        correct = tf.equal(pred, y)\n",
    "        correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\n",
    "\n",
    "        total_correct += int(correct)\n",
    "        total_num += x.shape[0]\n",
    "\n",
    "    acc = total_correct / total_num\n",
    "    print(epoch, 'test acc:', acc)"
   ]
  }
 ]
}