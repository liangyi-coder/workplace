{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0467d792f1dc256c793d370cee59531f961e826777ee61a5e454fc69acb25c662",
   "display_name": "Python 3.8.8 64-bit ('tensorflow': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "rf = tf.random.uniform(shape=())#定义一个随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.constant([[1.,2.],[3.,4.]])\n",
    "B = tf.tensor([[5.,6.],[7.,8.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.shape)\n",
    "print(A.dtype)\n",
    "print(A.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "broadcast : 用来自动扩张维度来达到可以进行维度间计算的目的,这种手段不会改变真实的维度，也不会发生复制，节省了内存\n",
    "'''\n",
    "A = tf.constant([[1],[2],[3],[4]], dtype=float)\n",
    "B = tf.constant([[5, 6, 7, 8]], dtype=float)\n",
    "print(A.shape, B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A + B #可以通过直接的计算来默认应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.shape, B.shape, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.add(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#或者用broadcast_to\n",
    "C = tf.broadcast_to(A,[4,8])\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "计算函数导数\n",
    "'''\n",
    "x = tf.Variable(initial_value=3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x)\n",
    "\n",
    "y_grad = tape.gradient(y, x)\n",
    "print(y, y_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多元求导\n",
    "import tensorflow as tf\n",
    "x = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable(initial_value=[[1.], [2.]])\n",
    "b = tf.Variable(initial_value=1.)\n",
    "with tf.GradientTape()as tape:\n",
    "    L = tf.reduce_sum(tf.square(tf.matmul(x, w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])\n",
    "print(L, w_grad, b_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "使用numpy实现一个简单的线性回归\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 0, 0 #为函数设置初始值\n",
    "epochs = 100000 #迭代总次数\n",
    "learning_rate = 1e-2#学习率\n",
    "#注意数据的归一化\n",
    "x_raw = (x_raw - x_raw.min()) / (x_raw.max() - x_raw.min())\n",
    "y_raw = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "m = len(x)\n",
    "def F(x, a, b):\n",
    "    y = x * a + b\n",
    "    return y\n",
    "\n",
    "def J_F(x_raw, y_raw, a, b, m):\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        cost += (F(x_raw[i], a, b) - y_raw[i])**2\n",
    "    cost = cost/(2 * m)\n",
    "    return cost\n",
    "last_cost = J_F(x_raw, y_raw, a, b, m)\n",
    "for e in range(epochs):\n",
    "    grad_a ,grad_b = 0, 0\n",
    "    for i in range(m):\n",
    "        grad_a += (F(x_raw[i], a, b) - y_raw[i])*x_raw[i]\n",
    "        grad_b += F(x_raw[i], a, b) - y_raw[i]\n",
    "    a = a - (grad_a/m)*learning_rate\n",
    "    b = b - (grad_b/m)*learning_rate\n",
    "    new_cost = J_F(x_raw, y_raw, a, b, m)\n",
    "    print(new_cost)\n",
    "    #if abs(last_cost - new_cost) < 0.01:\n",
    "    #   break\n",
    "    #last_cost = new_cost\n",
    "print(x_raw)\n",
    "print(y_raw)\n",
    "print(m, a, b)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a ,b = 0, 0\n",
    "num_epochs = 10000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    y_pred = a*x+b\n",
    "    grad_a, grad_b = 2 * (y_pred - y).dot(x), 2 * (y_pred - y).sum()\n",
    "    a, b = a - learning_rate * grad_a, b - learning_rate * grad_b\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "\n",
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variables = [a, b]\n",
    "\n",
    "num_epoch = 10000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=5e-4)\n",
    "for e in range(num_epoch):\n",
    "    # 使用tf.GradientTape()记录损失函数的梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b\n",
    "        loss = tf.reduce_sum(tf.square(y_pred - y))\n",
    "    # TensorFlow自动计算损失函数关于自变量（模型参数）的梯度\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    # TensorFlow自动根据梯度更新参数\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.9817748> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0545703>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "\n",
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variables = [a, b]\n",
    "\n",
    "num_epoch = 10000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    # 使用tf.GradientTape()记录损失函数的梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b\n",
    "        loss = tf.reduce_sum(tf.square(y_pred - y))\n",
    "    # TensorFlow自动计算损失函数关于自变量（模型参数）的梯度\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    # TensorFlow自动根据梯度更新参数\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}