{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function Description：使用现有的VGG16搭建一个神经网络以实现乳腺癌分子分型的分类\n",
    "'''\n",
    "import tensorflow as tf \n",
    "import os\n",
    "import numpy as np \n",
    "#加载训练数据\n",
    "#以下是train和test的图片集合\n",
    "batch_size = 20\n",
    "learning_rate = 1e-3\n",
    "class_names = ['Her2', 'luminal_A', 'luminal_B', 'TN']#每个分类的名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取train的文件路径\n",
    "train_data_dir = '/media/ly/liangyi/分子分型/X线MRI共有/MRI_minbox/train'\n",
    "validation_data_dir = '/media/ly/liangyi/分子分型/X线MRI共有/MRI_minbox/validation'\n",
    "Her2_train_data_dir = train_data_dir + '/Her_2/'\n",
    "luminal_A_train_data_dir = train_data_dir + '/luminal_A/'\n",
    "luminal_B_train_data_dir = train_data_dir + '/luminal_B/'\n",
    "TN_train_data_dir = train_data_dir + '/TN/'\n",
    "\n",
    "#将train的图片名依次读取\n",
    "Her2_train_data_name = [Her2_train_data_dir + filename for filename in os.listdir(Her2_train_data_dir)]\n",
    "luminal_A_train_data_name = [luminal_A_train_data_dir + filename for filename in os.listdir(luminal_A_train_data_dir)]\n",
    "luminal_B_train_data_name = [luminal_B_train_data_dir + filename for filename in os.listdir(luminal_B_train_data_dir)]\n",
    "TN_train_data_name = [TN_train_data_dir + filename for filename in os.listdir(TN_train_data_dir)]\n",
    "\n",
    "train_data_name = Her2_train_data_name + luminal_A_train_data_name + luminal_B_train_data_name + TN_train_data_name\n",
    "train_data_name = tf.convert_to_tensor(train_data_name)\n",
    "\n",
    "#用0,1,2,3分别代表四种类型\n",
    "train_data_labels = [0]* len(Her2_train_data_name) + [1]*len(luminal_A_train_data_name) + [2]*len(luminal_B_train_data_name) + [3] * len(TN_train_data_name)\n",
    "train_data_labels = tf.convert_to_tensor(train_data_labels, dtype=tf.int64)\n",
    "#train_data_name = np.array(train_data_name)\n",
    "#train_data_labels = np.array(train_data_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(583,)\n(583,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_name.shape)\n",
    "print(train_data_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "189 125 148 121\n"
     ]
    }
   ],
   "source": [
    "print(len(Her2_train_data_name), len(luminal_A_train_data_name), len(luminal_B_train_data_name), len(TN_train_data_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取验证图像的文家路径\n",
    "Her2_val_data_dir = validation_data_dir + '/Her_2/'\n",
    "luminal_A_val_data_dir = validation_data_dir + '/luminal_A/'\n",
    "luminal_B_val_data_dir = validation_data_dir + '/luminal_B/'\n",
    "TN_val_data_dir = validation_data_dir + '/TN/'\n",
    "#获取验证图像的文件名称\n",
    "Her2_val_data_name = [Her2_val_data_dir + filename for filename in os.listdir(Her2_val_data_dir)]\n",
    "luminal_A_val_data_name = [luminal_A_val_data_dir + filename for filename in os.listdir(luminal_A_val_data_dir)]\n",
    "luminal_B_val_data_name = [luminal_B_val_data_dir + filename for filename in os.listdir(luminal_B_val_data_dir)]\n",
    "TN_val_data_name = [TN_val_data_dir + filename for filename in os.listdir(TN_val_data_dir)]\n",
    "val_data_name = Her2_val_data_name + luminal_A_val_data_name + luminal_B_val_data_name + TN_val_data_name\n",
    "\n",
    "val_data_labels = [0] * len(Her2_val_data_name) + [1] * len(luminal_A_val_data_name) + [2] * len(luminal_B_val_data_name) + [3] * len(TN_val_data_name)\n",
    "#val_data_name = np.array(val_data_name)\n",
    "#val_data_labels = np.array(val_data_labels)\n",
    "val_data_name = tf.convert_to_tensor(val_data_name)\n",
    "val_data_labels = tf.convert_to_tensor(val_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(250,)\n(250,)\n"
     ]
    }
   ],
   "source": [
    "print(val_data_name.shape)\n",
    "print(val_data_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建映射函数\n",
    "def _map(imagename, label):\n",
    "    image_string = tf.io.read_file(imagename)\n",
    "    image_decoded = tf.image.decode_png(image_string, channels = 1)\n",
    "    image_resize = tf.image.resize(image_decoded, [28,28])/255.0\n",
    "    return image_resize, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data_name, train_data_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    map_func = _map,\n",
    "    num_parallel_calls= tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对训练集合做出设定， \n",
    "train_dataset = train_dataset.shuffle(buffer_size = 1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data_name, val_data_labels))\n",
    "val_dataset = val_dataset.map(_map).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PrefetchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)> <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset, val_dataset)# dataset 本身无法方便地获取元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation = tf.nn.relu, input_shape = (28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 5, activation = tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(4, activation = tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3749 - sparse_categorical_accuracy: 0.3087\n",
      "Epoch 2/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.3740 - sparse_categorical_accuracy: 0.3242\n",
      "Epoch 3/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.3699 - sparse_categorical_accuracy: 0.3242\n",
      "Epoch 4/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.3669 - sparse_categorical_accuracy: 0.3242\n",
      "Epoch 5/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.3650 - sparse_categorical_accuracy: 0.3242\n",
      "Epoch 6/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.3663 - sparse_categorical_accuracy: 0.3328\n",
      "Epoch 7/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.3473 - sparse_categorical_accuracy: 0.3293\n",
      "Epoch 8/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.3198 - sparse_categorical_accuracy: 0.3739\n",
      "Epoch 9/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.2905 - sparse_categorical_accuracy: 0.4134\n",
      "Epoch 10/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.2675 - sparse_categorical_accuracy: 0.4014\n",
      "Epoch 11/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.2617 - sparse_categorical_accuracy: 0.4168\n",
      "Epoch 12/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.2070 - sparse_categorical_accuracy: 0.4563\n",
      "Epoch 13/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.1896 - sparse_categorical_accuracy: 0.4425\n",
      "Epoch 14/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.1950 - sparse_categorical_accuracy: 0.4322\n",
      "Epoch 15/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.1429 - sparse_categorical_accuracy: 0.4717\n",
      "Epoch 16/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.1165 - sparse_categorical_accuracy: 0.5077\n",
      "Epoch 17/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.0843 - sparse_categorical_accuracy: 0.5111\n",
      "Epoch 18/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.0876 - sparse_categorical_accuracy: 0.4923\n",
      "Epoch 19/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.0551 - sparse_categorical_accuracy: 0.5146\n",
      "Epoch 20/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.0375 - sparse_categorical_accuracy: 0.5369\n",
      "Epoch 21/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.9935 - sparse_categorical_accuracy: 0.5523\n",
      "Epoch 22/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.9807 - sparse_categorical_accuracy: 0.5746\n",
      "Epoch 23/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.9781 - sparse_categorical_accuracy: 0.5678\n",
      "Epoch 24/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.9438 - sparse_categorical_accuracy: 0.5883\n",
      "Epoch 25/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.9087 - sparse_categorical_accuracy: 0.6312\n",
      "Epoch 26/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.8950 - sparse_categorical_accuracy: 0.6244\n",
      "Epoch 27/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.8843 - sparse_categorical_accuracy: 0.6312\n",
      "Epoch 28/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.8372 - sparse_categorical_accuracy: 0.6569\n",
      "Epoch 29/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.8468 - sparse_categorical_accuracy: 0.6501\n",
      "Epoch 30/30\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.7984 - sparse_categorical_accuracy: 0.6707\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe018ec3d50>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13/13 [==============================] - 0s 5ms/step - loss: 1.1285 - sparse_categorical_accuracy: 0.5200\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1.128541350364685, 0.5199999809265137]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "model.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, 3, activation = tf.nn.relu, input_shape = (28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation = tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation = tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(64, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(4, activation = tf.nn.sigmoid)\n",
    "])\n",
    "model1.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7830 - sparse_categorical_accuracy: 0.6518 - val_loss: 1.3824 - val_sparse_categorical_accuracy: 0.4840\n",
      "Epoch 2/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7997 - sparse_categorical_accuracy: 0.6329 - val_loss: 1.2272 - val_sparse_categorical_accuracy: 0.5320\n",
      "Epoch 3/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7417 - sparse_categorical_accuracy: 0.6604 - val_loss: 1.3101 - val_sparse_categorical_accuracy: 0.4800\n",
      "Epoch 4/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7030 - sparse_categorical_accuracy: 0.6861 - val_loss: 1.2978 - val_sparse_categorical_accuracy: 0.5520\n",
      "Epoch 5/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6495 - sparse_categorical_accuracy: 0.7238 - val_loss: 1.1758 - val_sparse_categorical_accuracy: 0.5640\n",
      "Epoch 6/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7644 - sparse_categorical_accuracy: 0.6724 - val_loss: 1.5192 - val_sparse_categorical_accuracy: 0.4520\n",
      "Epoch 7/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6485 - sparse_categorical_accuracy: 0.7204 - val_loss: 1.3417 - val_sparse_categorical_accuracy: 0.5160\n",
      "Epoch 8/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6276 - sparse_categorical_accuracy: 0.7393 - val_loss: 1.3118 - val_sparse_categorical_accuracy: 0.5440\n",
      "Epoch 9/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5418 - sparse_categorical_accuracy: 0.7822 - val_loss: 1.3162 - val_sparse_categorical_accuracy: 0.5480\n",
      "Epoch 10/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4827 - sparse_categorical_accuracy: 0.8062 - val_loss: 1.3345 - val_sparse_categorical_accuracy: 0.5640\n",
      "Epoch 11/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4961 - sparse_categorical_accuracy: 0.8027 - val_loss: 1.4855 - val_sparse_categorical_accuracy: 0.5320\n",
      "Epoch 12/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4701 - sparse_categorical_accuracy: 0.8062 - val_loss: 1.4068 - val_sparse_categorical_accuracy: 0.5600\n",
      "Epoch 13/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3955 - sparse_categorical_accuracy: 0.8353 - val_loss: 1.5958 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 14/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3961 - sparse_categorical_accuracy: 0.8336 - val_loss: 1.5049 - val_sparse_categorical_accuracy: 0.5600\n",
      "Epoch 15/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3377 - sparse_categorical_accuracy: 0.8542 - val_loss: 1.4430 - val_sparse_categorical_accuracy: 0.5960\n",
      "Epoch 16/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3235 - sparse_categorical_accuracy: 0.8696 - val_loss: 1.4420 - val_sparse_categorical_accuracy: 0.5760\n",
      "Epoch 17/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.2841 - sparse_categorical_accuracy: 0.8971 - val_loss: 1.5320 - val_sparse_categorical_accuracy: 0.5880\n",
      "Epoch 18/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.2406 - sparse_categorical_accuracy: 0.9091 - val_loss: 1.7479 - val_sparse_categorical_accuracy: 0.5360\n",
      "Epoch 19/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.2445 - sparse_categorical_accuracy: 0.9074 - val_loss: 1.6497 - val_sparse_categorical_accuracy: 0.5880\n",
      "Epoch 20/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1966 - sparse_categorical_accuracy: 0.9160 - val_loss: 1.6629 - val_sparse_categorical_accuracy: 0.5880\n",
      "Epoch 21/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1736 - sparse_categorical_accuracy: 0.9314 - val_loss: 2.1151 - val_sparse_categorical_accuracy: 0.5520\n",
      "Epoch 22/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.2910 - sparse_categorical_accuracy: 0.9074 - val_loss: 2.1452 - val_sparse_categorical_accuracy: 0.5360\n",
      "Epoch 23/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3362 - sparse_categorical_accuracy: 0.8765 - val_loss: 1.9840 - val_sparse_categorical_accuracy: 0.5520\n",
      "Epoch 24/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.2708 - sparse_categorical_accuracy: 0.9057 - val_loss: 1.9253 - val_sparse_categorical_accuracy: 0.5880\n",
      "Epoch 25/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1740 - sparse_categorical_accuracy: 0.9400 - val_loss: 1.7379 - val_sparse_categorical_accuracy: 0.5760\n",
      "Epoch 26/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1322 - sparse_categorical_accuracy: 0.9503 - val_loss: 1.8758 - val_sparse_categorical_accuracy: 0.5800\n",
      "Epoch 27/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0893 - sparse_categorical_accuracy: 0.9743 - val_loss: 1.8765 - val_sparse_categorical_accuracy: 0.5720\n",
      "Epoch 28/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0704 - sparse_categorical_accuracy: 0.9811 - val_loss: 1.8602 - val_sparse_categorical_accuracy: 0.6120\n",
      "Epoch 29/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0738 - sparse_categorical_accuracy: 0.9743 - val_loss: 1.9980 - val_sparse_categorical_accuracy: 0.5960\n",
      "Epoch 30/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0953 - sparse_categorical_accuracy: 0.9674 - val_loss: 2.1027 - val_sparse_categorical_accuracy: 0.5480\n",
      "Epoch 31/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0639 - sparse_categorical_accuracy: 0.9897 - val_loss: 2.0124 - val_sparse_categorical_accuracy: 0.5880\n",
      "Epoch 32/40\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0464 - sparse_categorical_accuracy: 0.9863 - val_loss: 2.0111 - val_sparse_categorical_accuracy: 0.5760\n",
      "Epoch 33/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0380 - sparse_categorical_accuracy: 0.9897 - val_loss: 2.0591 - val_sparse_categorical_accuracy: 0.5880\n",
      "Epoch 34/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0437 - sparse_categorical_accuracy: 0.9914 - val_loss: 2.1654 - val_sparse_categorical_accuracy: 0.5920\n",
      "Epoch 35/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1412 - sparse_categorical_accuracy: 0.9605 - val_loss: 2.0957 - val_sparse_categorical_accuracy: 0.5120\n",
      "Epoch 36/40\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0961 - sparse_categorical_accuracy: 0.9708 - val_loss: 2.3155 - val_sparse_categorical_accuracy: 0.5640\n",
      "Epoch 37/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1045 - sparse_categorical_accuracy: 0.9657 - val_loss: 2.2579 - val_sparse_categorical_accuracy: 0.5600\n",
      "Epoch 38/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3621 - sparse_categorical_accuracy: 0.8885 - val_loss: 2.4559 - val_sparse_categorical_accuracy: 0.5440\n",
      "Epoch 39/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.2229 - sparse_categorical_accuracy: 0.9125 - val_loss: 2.1394 - val_sparse_categorical_accuracy: 0.5680\n",
      "Epoch 40/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1652 - sparse_categorical_accuracy: 0.9383 - val_loss: 2.2557 - val_sparse_categorical_accuracy: 0.5440\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf244ae450>"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "model1.fit(train_dataset, validation_data = val_dataset, epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python379jvsc74a57bd03110ea37ffd25c4a84d6943605b7c0ed3a62606047d22cee2356eefcf32f74cc",
   "display_name": "Python 3.7.9 64-bit ('tensorflow2.3.1': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}