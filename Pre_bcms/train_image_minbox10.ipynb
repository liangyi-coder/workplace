{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd004ec051ae4b6de4caa85fff1145419125b9f654bedbf4d25524fd8efe58e477a",
   "display_name": "Python 3.6.13 64-bit ('tensorflow2.3.1': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Experiment Description： 使用MRIbydai_minbox（即加入一定像素的边缘），来进行分类\n",
    "DATA: train data 共600， 每类150； validation data 共233， 分类为\n",
    "'''\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import os \n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 1e-3\n",
    "class_names = ['Her2', 'luminal_A', 'luminal_B', 'TN']#每个分类的名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取data文件路径\n",
    "data_dir = 'F:/分子分型/X线MRIbydai_minbox/MRIbydai_minbox/'\n",
    "Her2_dir = data_dir + 'Her_2/'\n",
    "luminal_A_dir = data_dir + 'luminal_A/'\n",
    "luminal_B_dir = data_dir + 'luminal_B/'\n",
    "TN_dir = data_dir + 'TN/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "270 179 211 173\n"
     ]
    }
   ],
   "source": [
    "#读取每个类别下面图片的数量\n",
    "Her2_names = [Her2_dir + filename for filename in os.listdir(Her2_dir)]\n",
    "luminal_A_names = [luminal_A_dir + filename for filename in os.listdir(luminal_A_dir)]\n",
    "luminal_B_names = [luminal_B_dir + filename for filename in os.listdir(luminal_B_dir)]\n",
    "TN_names = [TN_dir + filename for filename in os.listdir(TN_dir)]\n",
    "\n",
    "print(len(Her2_names), len(luminal_A_names), len(luminal_B_names), len(TN_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#为了保证每类样本的train数量，每类样本训练数量分别提取前150个,共600\n",
    "Her2_train_names = Her2_names[0:150]\n",
    "luminal_A_train_names = luminal_A_names[0:150]\n",
    "luminal_B_train_names = luminal_B_names[0:150]\n",
    "TN_train_names = TN_names[0:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "train_names = Her2_train_names + luminal_A_train_names + luminal_B_train_names + TN_train_names\n",
    "print(len(train_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "#创建train_labels\n",
    "train_labels = [0] * len(Her2_train_names) + [1] * len(luminal_A_train_names) + [2] * len(luminal_B_train_names) + [3] * len(TN_train_names)\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_names, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _map(imagename, label):\n",
    "    image_string = tf.io.read_file(imagename)\n",
    "    image_decoded = tf.image.decode_png(image_string, channels=1)\n",
    "    image_resized = tf.image.resize(image_decoded, [40,40])/255.0\n",
    "    return image_resized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    map_func= _map,\n",
    "    num_parallel_calls= tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PrefetchDataset shapes: ((None, 40, 40, 1), (None,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "#检验数据是否倒入正确\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "#取余下数据集进行测试\n",
    "Her2_test_names = Her2_names[150:]\n",
    "luminal_A_test_names = luminal_A_names[150:]\n",
    "luminal_B_test_names = luminal_B_names[150:]\n",
    "TN_test_names = TN_names[150:]\n",
    "test_names = Her2_test_names + luminal_A_test_names + luminal_B_test_names + TN_test_names\n",
    "print(len(test_names)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "test_labels = [0] * len(Her2_test_names) + [1] * len(luminal_A_test_names) + [2] * len(luminal_B_test_names) + [3] * len(TN_test_names)\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BatchDataset shapes: ((None, 40, 40, 1), (None,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_names, test_labels))\n",
    "test_dataset = test_dataset.map(_map).batch(batch_size)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建网络\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, 3, activation = tf.nn.relu, input_shape = (40,40,1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation = tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation = tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(64, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4, activation = tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 38, 38, 64)        640       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 19, 19, 64)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 17, 17, 128)       73856     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 6, 6, 128)         147584    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 1152)              0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               295168    \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                16448     \n_________________________________________________________________\ndropout (Dropout)            (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 260       \n=================================================================\nTotal params: 533,956\nTrainable params: 533,956\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/40\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.3900 - sparse_categorical_accuracy: 0.2317 - val_loss: 1.3905 - val_sparse_categorical_accuracy: 0.1288\n",
      "Epoch 2/40\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.3852 - sparse_categorical_accuracy: 0.2533 - val_loss: 1.3921 - val_sparse_categorical_accuracy: 0.0815\n",
      "Epoch 3/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.3724 - sparse_categorical_accuracy: 0.3333 - val_loss: 1.3591 - val_sparse_categorical_accuracy: 0.2876\n",
      "Epoch 4/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.3214 - sparse_categorical_accuracy: 0.3883 - val_loss: 1.3244 - val_sparse_categorical_accuracy: 0.3047\n",
      "Epoch 5/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2655 - sparse_categorical_accuracy: 0.4217 - val_loss: 1.4016 - val_sparse_categorical_accuracy: 0.1974\n",
      "Epoch 6/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2005 - sparse_categorical_accuracy: 0.4383 - val_loss: 1.4324 - val_sparse_categorical_accuracy: 0.2361\n",
      "Epoch 7/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1724 - sparse_categorical_accuracy: 0.4467 - val_loss: 1.4559 - val_sparse_categorical_accuracy: 0.2103\n",
      "Epoch 8/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0845 - sparse_categorical_accuracy: 0.5283 - val_loss: 1.5118 - val_sparse_categorical_accuracy: 0.2704\n",
      "Epoch 9/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0147 - sparse_categorical_accuracy: 0.5600 - val_loss: 1.7274 - val_sparse_categorical_accuracy: 0.2275\n",
      "Epoch 10/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.9177 - sparse_categorical_accuracy: 0.6200 - val_loss: 1.8543 - val_sparse_categorical_accuracy: 0.2275\n",
      "Epoch 11/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8058 - sparse_categorical_accuracy: 0.6683 - val_loss: 1.7803 - val_sparse_categorical_accuracy: 0.2961\n",
      "Epoch 12/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7317 - sparse_categorical_accuracy: 0.6850 - val_loss: 2.3084 - val_sparse_categorical_accuracy: 0.3047\n",
      "Epoch 13/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6340 - sparse_categorical_accuracy: 0.7667 - val_loss: 2.4390 - val_sparse_categorical_accuracy: 0.2489\n",
      "Epoch 14/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4882 - sparse_categorical_accuracy: 0.8083 - val_loss: 1.8335 - val_sparse_categorical_accuracy: 0.4077\n",
      "Epoch 15/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3795 - sparse_categorical_accuracy: 0.8717 - val_loss: 2.3816 - val_sparse_categorical_accuracy: 0.4034\n",
      "Epoch 16/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3135 - sparse_categorical_accuracy: 0.8967 - val_loss: 2.4755 - val_sparse_categorical_accuracy: 0.4249\n",
      "Epoch 17/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.2432 - sparse_categorical_accuracy: 0.9017 - val_loss: 2.9960 - val_sparse_categorical_accuracy: 0.3605\n",
      "Epoch 18/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1886 - sparse_categorical_accuracy: 0.9467 - val_loss: 3.9019 - val_sparse_categorical_accuracy: 0.3777\n",
      "Epoch 19/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1569 - sparse_categorical_accuracy: 0.9500 - val_loss: 4.2112 - val_sparse_categorical_accuracy: 0.3004\n",
      "Epoch 20/40\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0881 - sparse_categorical_accuracy: 0.9767 - val_loss: 4.1356 - val_sparse_categorical_accuracy: 0.4077\n",
      "Epoch 21/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0773 - sparse_categorical_accuracy: 0.9783 - val_loss: 5.0710 - val_sparse_categorical_accuracy: 0.3133\n",
      "Epoch 22/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1063 - sparse_categorical_accuracy: 0.9667 - val_loss: 4.3852 - val_sparse_categorical_accuracy: 0.3562\n",
      "Epoch 23/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0791 - sparse_categorical_accuracy: 0.9783 - val_loss: 5.3883 - val_sparse_categorical_accuracy: 0.3262\n",
      "Epoch 24/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0984 - sparse_categorical_accuracy: 0.9733 - val_loss: 3.8050 - val_sparse_categorical_accuracy: 0.4077\n",
      "Epoch 25/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1249 - sparse_categorical_accuracy: 0.9567 - val_loss: 4.5255 - val_sparse_categorical_accuracy: 0.3348\n",
      "Epoch 26/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0717 - sparse_categorical_accuracy: 0.9800 - val_loss: 4.4541 - val_sparse_categorical_accuracy: 0.3476\n",
      "Epoch 27/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0511 - sparse_categorical_accuracy: 0.9917 - val_loss: 5.3751 - val_sparse_categorical_accuracy: 0.3047\n",
      "Epoch 28/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0648 - sparse_categorical_accuracy: 0.9750 - val_loss: 5.6337 - val_sparse_categorical_accuracy: 0.2232\n",
      "Epoch 29/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0707 - sparse_categorical_accuracy: 0.9750 - val_loss: 4.2769 - val_sparse_categorical_accuracy: 0.4077\n",
      "Epoch 30/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0809 - sparse_categorical_accuracy: 0.9767 - val_loss: 4.4117 - val_sparse_categorical_accuracy: 0.3519\n",
      "Epoch 31/40\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0376 - sparse_categorical_accuracy: 0.9950 - val_loss: 5.6397 - val_sparse_categorical_accuracy: 0.2833\n",
      "Epoch 32/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0579 - sparse_categorical_accuracy: 0.9867 - val_loss: 4.5187 - val_sparse_categorical_accuracy: 0.3519\n",
      "Epoch 33/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0113 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.5135 - val_sparse_categorical_accuracy: 0.3820\n",
      "Epoch 34/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9967 - val_loss: 5.0348 - val_sparse_categorical_accuracy: 0.3820\n",
      "Epoch 35/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0341 - sparse_categorical_accuracy: 0.9933 - val_loss: 5.5881 - val_sparse_categorical_accuracy: 0.3047\n",
      "Epoch 36/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1617 - sparse_categorical_accuracy: 0.9567 - val_loss: 4.8958 - val_sparse_categorical_accuracy: 0.2318\n",
      "Epoch 37/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0706 - sparse_categorical_accuracy: 0.9800 - val_loss: 4.1963 - val_sparse_categorical_accuracy: 0.3648\n",
      "Epoch 38/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9983 - val_loss: 4.3605 - val_sparse_categorical_accuracy: 0.4077\n",
      "Epoch 39/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9950 - val_loss: 5.2094 - val_sparse_categorical_accuracy: 0.3391\n",
      "Epoch 40/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0104 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.0914 - val_sparse_categorical_accuracy: 0.3562\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1afa90d2128>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data=test_dataset, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation = tf.nn.relu, input_shape = (40,40,1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 5, activation = tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(4, activation = tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/40\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.3776 - sparse_categorical_accuracy: 0.3050 - val_loss: 1.4387 - val_sparse_categorical_accuracy: 0.1159\n",
      "Epoch 2/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3424 - sparse_categorical_accuracy: 0.3550 - val_loss: 1.3920 - val_sparse_categorical_accuracy: 0.1116\n",
      "Epoch 3/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2558 - sparse_categorical_accuracy: 0.4300 - val_loss: 1.5045 - val_sparse_categorical_accuracy: 0.2060\n",
      "Epoch 4/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1395 - sparse_categorical_accuracy: 0.4667 - val_loss: 1.2982 - val_sparse_categorical_accuracy: 0.3734\n",
      "Epoch 5/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9961 - sparse_categorical_accuracy: 0.5867 - val_loss: 1.5825 - val_sparse_categorical_accuracy: 0.3391\n",
      "Epoch 6/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9237 - sparse_categorical_accuracy: 0.6133 - val_loss: 1.4389 - val_sparse_categorical_accuracy: 0.3777\n",
      "Epoch 7/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8131 - sparse_categorical_accuracy: 0.6917 - val_loss: 2.1530 - val_sparse_categorical_accuracy: 0.2918\n",
      "Epoch 8/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6377 - sparse_categorical_accuracy: 0.7800 - val_loss: 1.6662 - val_sparse_categorical_accuracy: 0.4163\n",
      "Epoch 9/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5242 - sparse_categorical_accuracy: 0.8050 - val_loss: 2.7490 - val_sparse_categorical_accuracy: 0.2661\n",
      "Epoch 10/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.4517 - sparse_categorical_accuracy: 0.8400 - val_loss: 2.2750 - val_sparse_categorical_accuracy: 0.3391\n",
      "Epoch 11/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2952 - sparse_categorical_accuracy: 0.9083 - val_loss: 2.5952 - val_sparse_categorical_accuracy: 0.3605\n",
      "Epoch 12/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2416 - sparse_categorical_accuracy: 0.9417 - val_loss: 2.6633 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 13/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2371 - sparse_categorical_accuracy: 0.9233 - val_loss: 2.8899 - val_sparse_categorical_accuracy: 0.3433\n",
      "Epoch 14/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1459 - sparse_categorical_accuracy: 0.9667 - val_loss: 2.9880 - val_sparse_categorical_accuracy: 0.3519\n",
      "Epoch 15/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1019 - sparse_categorical_accuracy: 0.9750 - val_loss: 2.8258 - val_sparse_categorical_accuracy: 0.3648\n",
      "Epoch 16/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0913 - sparse_categorical_accuracy: 0.9800 - val_loss: 3.0709 - val_sparse_categorical_accuracy: 0.3906\n",
      "Epoch 17/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0755 - sparse_categorical_accuracy: 0.9867 - val_loss: 3.3010 - val_sparse_categorical_accuracy: 0.3605\n",
      "Epoch 18/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0472 - sparse_categorical_accuracy: 0.9967 - val_loss: 3.5627 - val_sparse_categorical_accuracy: 0.3476\n",
      "Epoch 19/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0308 - sparse_categorical_accuracy: 0.9983 - val_loss: 3.6323 - val_sparse_categorical_accuracy: 0.3691\n",
      "Epoch 20/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0246 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.5512 - val_sparse_categorical_accuracy: 0.3734\n",
      "Epoch 21/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0228 - sparse_categorical_accuracy: 0.9983 - val_loss: 3.9140 - val_sparse_categorical_accuracy: 0.3734\n",
      "Epoch 22/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0256 - sparse_categorical_accuracy: 0.9950 - val_loss: 3.8512 - val_sparse_categorical_accuracy: 0.3820\n",
      "Epoch 23/40\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0162 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.8056 - val_sparse_categorical_accuracy: 0.3648\n",
      "Epoch 24/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0107 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.9211 - val_sparse_categorical_accuracy: 0.3777\n",
      "Epoch 25/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0097 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.8595 - val_sparse_categorical_accuracy: 0.3820\n",
      "Epoch 26/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0075 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.9758 - val_sparse_categorical_accuracy: 0.3648\n",
      "Epoch 27/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0074 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.9097 - val_sparse_categorical_accuracy: 0.3948\n",
      "Epoch 28/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0066 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0251 - val_sparse_categorical_accuracy: 0.3820\n",
      "Epoch 29/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0046 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0574 - val_sparse_categorical_accuracy: 0.3820\n",
      "Epoch 30/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0040 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0732 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 31/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0038 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1618 - val_sparse_categorical_accuracy: 0.3734\n",
      "Epoch 32/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0036 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1266 - val_sparse_categorical_accuracy: 0.3820\n",
      "Epoch 33/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0032 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1824 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 34/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0029 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2314 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 35/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0028 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1820 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 36/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2778 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 37/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0023 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.3124 - val_sparse_categorical_accuracy: 0.3906\n",
      "Epoch 38/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0023 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.3064 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 39/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0021 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.3047 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 40/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0019 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2926 - val_sparse_categorical_accuracy: 0.3906\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1af9688ea90>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "model1.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n",
    ")\n",
    "model1.fit(train_dataset, validation_data=test_dataset, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}