{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd03110ea37ffd25c4a84d6943605b7c0ed3a62606047d22cee2356eefcf32f74cc",
   "display_name": "Python 3.7.9 64-bit ('tensorflow2.3.1': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Experiment Description： 使用MRIbydai_minbox（即加入一定像素的边缘），来进行分类\n",
    "DATA: train data 共600， 每类150； validation data 共233， 分类为\n",
    "'''\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import os \n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 1e-3\n",
    "class_names = ['Her2', 'luminal_A', 'luminal_B', 'TN']#每个分类的名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取data文件路径\n",
    "data_dir = '/media/ly/liangyi/研究课题——乳腺癌分子分型分类/Data/MRIbydai_minbox/'\n",
    "Her2_dir = data_dir + 'Her_2/'\n",
    "luminal_A_dir = data_dir + 'luminal_A/'\n",
    "luminal_B_dir = data_dir + 'luminal_B/'\n",
    "TN_dir = data_dir + 'TN/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "270 179 211 173\n"
     ]
    }
   ],
   "source": [
    "#读取每个类别下面图片的数量\n",
    "Her2_names = [Her2_dir + filename for filename in os.listdir(Her2_dir)]\n",
    "luminal_A_names = [luminal_A_dir + filename for filename in os.listdir(luminal_A_dir)]\n",
    "luminal_B_names = [luminal_B_dir + filename for filename in os.listdir(luminal_B_dir)]\n",
    "TN_names = [TN_dir + filename for filename in os.listdir(TN_dir)]\n",
    "\n",
    "print(len(Her2_names), len(luminal_A_names), len(luminal_B_names), len(TN_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#为了保证每类样本的train数量，每类样本训练数量分别提取前150个,共600\n",
    "Her2_train_names = Her2_names[0:150]\n",
    "luminal_A_train_names = luminal_A_names[0:150]\n",
    "luminal_B_train_names = luminal_B_names[0:150]\n",
    "TN_train_names = TN_names[0:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "train_names = Her2_train_names + luminal_A_train_names + luminal_B_train_names + TN_train_names\n",
    "print(len(train_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "#创建train_labels\n",
    "train_labels = [0] * len(Her2_train_names) + [1] * len(luminal_A_train_names) + [2] * len(luminal_B_train_names) + [3] * len(TN_train_names)\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_names, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _map(imagename, label):\n",
    "    image_string = tf.io.read_file(imagename)\n",
    "    image_decoded = tf.image.decode_png(image_string, channels=1)\n",
    "    image_resized = tf.image.resize(image_decoded, [40,40])/255.0\n",
    "    return image_resized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    map_func= _map,\n",
    "    num_parallel_calls= tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PrefetchDataset shapes: ((None, 40, 40, 1), (None,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "#检验数据是否倒入正确\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "#取余下数据集进行测试\n",
    "Her2_test_names = Her2_names[150:]\n",
    "luminal_A_test_names = luminal_A_names[150:]\n",
    "luminal_B_test_names = luminal_B_names[150:]\n",
    "TN_test_names = TN_names[150:]\n",
    "test_names = Her2_test_names + luminal_A_test_names + luminal_B_test_names + TN_test_names\n",
    "print(len(test_names)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "test_labels = [0] * len(Her2_test_names) + [1] * len(luminal_A_test_names) + [2] * len(luminal_B_test_names) + [3] * len(TN_test_names)\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BatchDataset shapes: ((None, 40, 40, 1), (None,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_names, test_labels))\n",
    "test_dataset = test_dataset.map(_map).batch(batch_size)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建网络\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, 3, activation = tf.nn.relu, input_shape = (40,40,1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation = tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation = tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(64, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4, activation = tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 38, 38, 64)        640       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 19, 19, 64)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 17, 17, 128)       73856     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 6, 6, 128)         147584    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 1152)              0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               295168    \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                16448     \n_________________________________________________________________\ndropout (Dropout)            (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 260       \n=================================================================\nTotal params: 533,956\nTrainable params: 533,956\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/40\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.3891 - sparse_categorical_accuracy: 0.2250 - val_loss: 1.3874 - val_sparse_categorical_accuracy: 0.1631\n",
      "Epoch 2/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3848 - sparse_categorical_accuracy: 0.2633 - val_loss: 1.3934 - val_sparse_categorical_accuracy: 0.0987\n",
      "Epoch 3/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3756 - sparse_categorical_accuracy: 0.2883 - val_loss: 1.3449 - val_sparse_categorical_accuracy: 0.3648\n",
      "Epoch 4/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3426 - sparse_categorical_accuracy: 0.3283 - val_loss: 1.3376 - val_sparse_categorical_accuracy: 0.1459\n",
      "Epoch 5/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3149 - sparse_categorical_accuracy: 0.3733 - val_loss: 1.4450 - val_sparse_categorical_accuracy: 0.0987\n",
      "Epoch 6/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2671 - sparse_categorical_accuracy: 0.4200 - val_loss: 1.2829 - val_sparse_categorical_accuracy: 0.4120\n",
      "Epoch 7/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2324 - sparse_categorical_accuracy: 0.4217 - val_loss: 1.3789 - val_sparse_categorical_accuracy: 0.2361\n",
      "Epoch 8/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1890 - sparse_categorical_accuracy: 0.4450 - val_loss: 1.4273 - val_sparse_categorical_accuracy: 0.3133\n",
      "Epoch 9/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0970 - sparse_categorical_accuracy: 0.4950 - val_loss: 1.4262 - val_sparse_categorical_accuracy: 0.3262\n",
      "Epoch 10/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0856 - sparse_categorical_accuracy: 0.5033 - val_loss: 1.7977 - val_sparse_categorical_accuracy: 0.1760\n",
      "Epoch 11/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9637 - sparse_categorical_accuracy: 0.5783 - val_loss: 1.8627 - val_sparse_categorical_accuracy: 0.2575\n",
      "Epoch 12/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9136 - sparse_categorical_accuracy: 0.6250 - val_loss: 1.7168 - val_sparse_categorical_accuracy: 0.3906\n",
      "Epoch 13/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7824 - sparse_categorical_accuracy: 0.6617 - val_loss: 1.8039 - val_sparse_categorical_accuracy: 0.3991\n",
      "Epoch 14/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7148 - sparse_categorical_accuracy: 0.6800 - val_loss: 1.7938 - val_sparse_categorical_accuracy: 0.3777\n",
      "Epoch 15/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5732 - sparse_categorical_accuracy: 0.7917 - val_loss: 1.9778 - val_sparse_categorical_accuracy: 0.3734\n",
      "Epoch 16/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.4557 - sparse_categorical_accuracy: 0.8033 - val_loss: 2.4731 - val_sparse_categorical_accuracy: 0.4421\n",
      "Epoch 17/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.4428 - sparse_categorical_accuracy: 0.8317 - val_loss: 2.0964 - val_sparse_categorical_accuracy: 0.4292\n",
      "Epoch 18/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3380 - sparse_categorical_accuracy: 0.8917 - val_loss: 3.0754 - val_sparse_categorical_accuracy: 0.3734\n",
      "Epoch 19/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3042 - sparse_categorical_accuracy: 0.9067 - val_loss: 3.1282 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 20/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2500 - sparse_categorical_accuracy: 0.9133 - val_loss: 3.3215 - val_sparse_categorical_accuracy: 0.3476\n",
      "Epoch 21/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2016 - sparse_categorical_accuracy: 0.9400 - val_loss: 3.3848 - val_sparse_categorical_accuracy: 0.4206\n",
      "Epoch 22/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1270 - sparse_categorical_accuracy: 0.9667 - val_loss: 4.0752 - val_sparse_categorical_accuracy: 0.3906\n",
      "Epoch 23/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1751 - sparse_categorical_accuracy: 0.9383 - val_loss: 2.9231 - val_sparse_categorical_accuracy: 0.4163\n",
      "Epoch 24/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1009 - sparse_categorical_accuracy: 0.9750 - val_loss: 4.5367 - val_sparse_categorical_accuracy: 0.3133\n",
      "Epoch 25/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1011 - sparse_categorical_accuracy: 0.9650 - val_loss: 4.3717 - val_sparse_categorical_accuracy: 0.3047\n",
      "Epoch 26/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1093 - sparse_categorical_accuracy: 0.9683 - val_loss: 3.4543 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 27/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1106 - sparse_categorical_accuracy: 0.9683 - val_loss: 3.6412 - val_sparse_categorical_accuracy: 0.3906\n",
      "Epoch 28/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0798 - sparse_categorical_accuracy: 0.9783 - val_loss: 3.8470 - val_sparse_categorical_accuracy: 0.3820\n",
      "Epoch 29/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0603 - sparse_categorical_accuracy: 0.9767 - val_loss: 4.2303 - val_sparse_categorical_accuracy: 0.4292\n",
      "Epoch 30/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1000 - sparse_categorical_accuracy: 0.9667 - val_loss: 3.4317 - val_sparse_categorical_accuracy: 0.4206\n",
      "Epoch 31/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0850 - sparse_categorical_accuracy: 0.9783 - val_loss: 3.8425 - val_sparse_categorical_accuracy: 0.3906\n",
      "Epoch 32/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0497 - sparse_categorical_accuracy: 0.9900 - val_loss: 4.0255 - val_sparse_categorical_accuracy: 0.4034\n",
      "Epoch 33/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0467 - sparse_categorical_accuracy: 0.9900 - val_loss: 4.4279 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 34/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0277 - sparse_categorical_accuracy: 0.9933 - val_loss: 4.2691 - val_sparse_categorical_accuracy: 0.3991\n",
      "Epoch 35/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9967 - val_loss: 4.6277 - val_sparse_categorical_accuracy: 0.3648\n",
      "Epoch 36/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0119 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.5447 - val_sparse_categorical_accuracy: 0.4034\n",
      "Epoch 37/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0096 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.0443 - val_sparse_categorical_accuracy: 0.3433\n",
      "Epoch 38/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9983 - val_loss: 4.7956 - val_sparse_categorical_accuracy: 0.3820\n",
      "Epoch 39/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0074 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.1204 - val_sparse_categorical_accuracy: 0.3519\n",
      "Epoch 40/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0163 - sparse_categorical_accuracy: 0.9967 - val_loss: 4.8983 - val_sparse_categorical_accuracy: 0.3648\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f83fc102f10>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data=test_dataset, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation = tf.nn.relu, input_shape = (40,40,1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 5, activation = tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(4, activation = tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3675 - sparse_categorical_accuracy: 0.3200 - val_loss: 1.4237 - val_sparse_categorical_accuracy: 0.1502\n",
      "Epoch 2/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3174 - sparse_categorical_accuracy: 0.3750 - val_loss: 1.4376 - val_sparse_categorical_accuracy: 0.2747\n",
      "Epoch 3/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.2148 - sparse_categorical_accuracy: 0.4650 - val_loss: 1.3903 - val_sparse_categorical_accuracy: 0.4292\n",
      "Epoch 4/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1380 - sparse_categorical_accuracy: 0.5283 - val_loss: 1.6127 - val_sparse_categorical_accuracy: 0.2489\n",
      "Epoch 5/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1186 - sparse_categorical_accuracy: 0.5417 - val_loss: 1.5146 - val_sparse_categorical_accuracy: 0.3047\n",
      "Epoch 6/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9717 - sparse_categorical_accuracy: 0.6067 - val_loss: 1.5407 - val_sparse_categorical_accuracy: 0.2532\n",
      "Epoch 7/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8277 - sparse_categorical_accuracy: 0.6750 - val_loss: 1.6034 - val_sparse_categorical_accuracy: 0.2833\n",
      "Epoch 8/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6909 - sparse_categorical_accuracy: 0.7633 - val_loss: 1.6295 - val_sparse_categorical_accuracy: 0.3605\n",
      "Epoch 9/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5644 - sparse_categorical_accuracy: 0.8017 - val_loss: 1.9377 - val_sparse_categorical_accuracy: 0.3176\n",
      "Epoch 10/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5117 - sparse_categorical_accuracy: 0.8267 - val_loss: 1.6468 - val_sparse_categorical_accuracy: 0.3948\n",
      "Epoch 11/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3808 - sparse_categorical_accuracy: 0.8783 - val_loss: 2.0624 - val_sparse_categorical_accuracy: 0.3691\n",
      "Epoch 12/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3012 - sparse_categorical_accuracy: 0.9017 - val_loss: 2.1165 - val_sparse_categorical_accuracy: 0.3948\n",
      "Epoch 13/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.2309 - sparse_categorical_accuracy: 0.9267 - val_loss: 2.3895 - val_sparse_categorical_accuracy: 0.3948\n",
      "Epoch 14/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1813 - sparse_categorical_accuracy: 0.9417 - val_loss: 2.9710 - val_sparse_categorical_accuracy: 0.3133\n",
      "Epoch 15/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1874 - sparse_categorical_accuracy: 0.9467 - val_loss: 2.3737 - val_sparse_categorical_accuracy: 0.3906\n",
      "Epoch 16/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1142 - sparse_categorical_accuracy: 0.9767 - val_loss: 2.5725 - val_sparse_categorical_accuracy: 0.4549\n",
      "Epoch 17/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1477 - sparse_categorical_accuracy: 0.9617 - val_loss: 3.0528 - val_sparse_categorical_accuracy: 0.3519\n",
      "Epoch 18/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0927 - sparse_categorical_accuracy: 0.9783 - val_loss: 2.8889 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 19/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0519 - sparse_categorical_accuracy: 0.9900 - val_loss: 2.6545 - val_sparse_categorical_accuracy: 0.4678\n",
      "Epoch 20/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0454 - sparse_categorical_accuracy: 0.9933 - val_loss: 2.9109 - val_sparse_categorical_accuracy: 0.4464\n",
      "Epoch 21/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0311 - sparse_categorical_accuracy: 0.9983 - val_loss: 2.6832 - val_sparse_categorical_accuracy: 0.4464\n",
      "Epoch 22/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0232 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.1451 - val_sparse_categorical_accuracy: 0.3863\n",
      "Epoch 23/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0201 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.9139 - val_sparse_categorical_accuracy: 0.4421\n",
      "Epoch 24/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0151 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.9948 - val_sparse_categorical_accuracy: 0.4206\n",
      "Epoch 25/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0147 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.7979 - val_sparse_categorical_accuracy: 0.4635\n",
      "Epoch 26/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0111 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.8927 - val_sparse_categorical_accuracy: 0.4807\n",
      "Epoch 27/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0105 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.2631 - val_sparse_categorical_accuracy: 0.4034\n",
      "Epoch 28/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0086 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.2713 - val_sparse_categorical_accuracy: 0.4163\n",
      "Epoch 29/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0075 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.1233 - val_sparse_categorical_accuracy: 0.4421\n",
      "Epoch 30/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0070 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.1826 - val_sparse_categorical_accuracy: 0.4378\n",
      "Epoch 31/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0057 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.1962 - val_sparse_categorical_accuracy: 0.4464\n",
      "Epoch 32/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0051 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.1872 - val_sparse_categorical_accuracy: 0.4464\n",
      "Epoch 33/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0048 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.2023 - val_sparse_categorical_accuracy: 0.4421\n",
      "Epoch 34/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0044 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.3143 - val_sparse_categorical_accuracy: 0.4249\n",
      "Epoch 35/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0041 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.2380 - val_sparse_categorical_accuracy: 0.4506\n",
      "Epoch 36/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0034 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.3583 - val_sparse_categorical_accuracy: 0.4378\n",
      "Epoch 37/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0034 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.3581 - val_sparse_categorical_accuracy: 0.4292\n",
      "Epoch 38/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0031 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.4316 - val_sparse_categorical_accuracy: 0.4249\n",
      "Epoch 39/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0030 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.2847 - val_sparse_categorical_accuracy: 0.4506\n",
      "Epoch 40/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.3627 - val_sparse_categorical_accuracy: 0.4421\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f83780972d0>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "model1.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n",
    ")\n",
    "model1.fit(train_dataset, validation_data=test_dataset, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}